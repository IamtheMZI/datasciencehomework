{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.1 nltk==3.5 colorama==0.4.3 numpy==1.18.5 scikit_learn==0.23.2 Flask==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Intent File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Intent.json',encoding=\"utf-8\") as file:\n",
    "    intent_data_json = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "response_json = {}\n",
    "for intent in intent_data_json['intents']:\n",
    "    for pattern in intent['text']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['intent'])\n",
    "\n",
    "    response_json[intent['intent']] = intent['responses']\n",
    "    \n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['intent'] not in labels:\n",
    "        labels.append(intent['intent'])\n",
    "\n",
    "with open('Intent_response.json','w',encoding=\"utf-8\") as fp:\n",
    "    json.dump(response_json, fp,indent=4)\n",
    "\n",
    "intent_data_pd = pd.json_normalize(intent_data_json, record_path =['intents'])\n",
    "common_question_words = [\"What\",\"When\",\"Which\",\"How\",\"Where\",\"What\",\"Is\",\"Am\",\"Are\",\"Was\",\"Were\",\"Shall\",\"Should\",\"Can\",\"Could\",\"Will\",\"Might\",\"Must\",\"a\",\"an\",\"the\"]\n",
    "common_question_words=  [i.lower() for i in common_question_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_labels, training_sentences):\n",
    "    num_classes = len(np.unique(training_labels))\n",
    "    lbl_encoder = LabelEncoder()\n",
    "    lbl_encoder.fit(training_labels)\n",
    "    training_labels = lbl_encoder.transform(training_labels)\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_len = 20\n",
    "    oov_token = \"<OOV>\"\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    epochs = 500\n",
    "    \n",
    "#     old_stdout = sys.stdout # backup current stdout\n",
    "#     sys.stdout = open(os.devnull, \"w\")\n",
    "    history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs, verbose='1')\n",
    "#     sys.stdout = old_stdout # reset old stdout\n",
    "\n",
    "    model.save(\"chat_model\")\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    # to save the fitted tokenizer\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # to save the fitted label encoder\n",
    "    with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "        pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train(training_labels, training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_user_name(input_query):\n",
    "    global user_data\n",
    "    user_data[\"User\"][\"Name\"] = input_query.split()[-1]\n",
    "    return None\n",
    "def get_user_name():\n",
    "    global user_data\n",
    "    return \"Your name is \" + user_data[\"User\"][\"Name\"] if user_data[\"User\"][\"Name\"] else \"Can you tell me your name?\"\n",
    "def greet_by_name():\n",
    "    global user_data\n",
    "    return \"Nice to Meet you \" + user_data[\"User\"][\"Name\"]\n",
    "def do_nothing():\n",
    "    pass\n",
    "def learn_new_question():\n",
    "    global training_sentences\n",
    "    global training_labels\n",
    "    global response_json\n",
    "    global intent_data_pd\n",
    "    print(Fore.GREEN + \"ChatBot:\" + str(\"[learnFunc]\") + Style.RESET_ALL , \"Can you tell me the question?\")\n",
    "    print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "    sentence = input()\n",
    "    training_sentences.append(sentence)\n",
    "    print(Fore.GREEN + \"ChatBot:\" + str(\"[learnFunc]\") + Style.RESET_ALL , \"Great. What is the answer?\")\n",
    "    print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "    answer = input()\n",
    "    print(Fore.GREEN + \"ChatBot:\" + str(\"[learnFunc]\") + Style.RESET_ALL , \"Last Question. What is the category?\")\n",
    "    print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "    label = input()\n",
    "    training_labels.append(label)\n",
    "    response_json[label] = response_json.get(label,[])\n",
    "    response_json[label].append(answer)\n",
    "    print(Fore.GREEN + \"ChatBot:\" + str(\"[learnFunc]\") + Style.RESET_ALL , \"Thanks. I will update my database soon!\")\n",
    "    \n",
    "    if not intent_data_pd[intent_data_pd.intent == label].empty:\n",
    "        intent_data_pd[intent_data_pd.intent == label][\"text\"].iloc[0].append(sentence)\n",
    "        intent_data_pd[intent_data_pd.intent == label][\"responses\"].iloc[0].append(answer)\n",
    "    else:\n",
    "        new_df = pd.DataFrame([[label,[sentence],[answer]]],columns=[\"intent\",\"text\",\"responses\"])\n",
    "        frames = [intent_data_pd,new_df]\n",
    "        intent_data_pd = pd.concat(frames,ignore_index=True)\n",
    "    store_current_data()\n",
    "def store_current_data():\n",
    "    global intent_data_pd\n",
    "    global user_data\n",
    "    with open(\"Intent.json\",\"w\") as output_file:\n",
    "        new_json = {}\n",
    "        new_json[\"intents\"] = json.loads(intent_data_pd.to_json(orient=\"records\"))\n",
    "        json.dump(new_json, output_file, indent=4)\n",
    "    with open(\"User.json\",\"w\") as output_file:\n",
    "        json.dump(user_data,output_file,indent=4)\n",
    "        \n",
    "def sanitize_what_is(input_query):\n",
    "    global common_question_words\n",
    "    \n",
    "    list_of_words = input_query.split()\n",
    "    for word in list_of_words:\n",
    "        if word in common_question_words:\n",
    "            list_of_words.remove(word)\n",
    "    return ''.join(str(i) for i in list_of_words)\n",
    "\n",
    "def what_is_lookup(input_query):\n",
    "    global user_data\n",
    "    key = sanitize_what_is(input_query.lower())\n",
    "    if not user_data.get(key,None):\n",
    "        print(Fore.GREEN + \"ChatBot:\" + str(\"[learnFunc]\") + Style.RESET_ALL , \"I am afraid I don't know the answer. What is the answer?\")\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        answer = input()\n",
    "        user_data[key] = answer\n",
    "        store_current_data()\n",
    "    return user_data[key]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def update_model():\n",
    "    global training_sentences\n",
    "    global training_labels\n",
    "    global response_json\n",
    "    train(training_sentences, training_labels)\n",
    "    load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import colorama \n",
    "colorama.init()\n",
    "from colorama import Fore, Style, Back\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "with open(\"Intent_response.json\",encoding=\"utf-8\") as intent_file:\n",
    "    data = json.load(intent_file)\n",
    "with open(\"User.json\", encoding=\"utf-8\") as user_file:\n",
    "    user_data = json.load(user_file)\n",
    "\n",
    "def load_model():\n",
    "    # load trained model\n",
    "    model = keras.models.load_model('chat_model')\n",
    "\n",
    "    # load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "    with open('label_encoder.pickle', 'rb') as enc:\n",
    "        lbl_encoder = pickle.load(enc)\n",
    "    return model,tokenizer,lbl_encoder\n",
    "\n",
    "def chat():   \n",
    "    model,tokenizer,lbl_encoder = load_model()\n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    update_response = False\n",
    "    last_query = ''\n",
    "    \n",
    "    iters = 100\n",
    "    while iters:\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        inp = input()\n",
    "        if inp.lower() == \"quit\" or inp.lower() == \"exit\" or inp.lower() == \"bye\":\n",
    "            break\n",
    "\n",
    "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),truncating='post', maxlen=max_len))\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "#             print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,tag)\n",
    "        iters = iters - 1\n",
    "        for key,val in response_json.items():\n",
    "#                 if tag == 'feedback':\n",
    "#                     update_response = True\n",
    "            if key == tag:\n",
    "                if \"Func\" in str(tag):\n",
    "                    resp_func = eval(val[0])\n",
    "                    _,resp = resp_func(inp)\n",
    "                else:\n",
    "                    resp = np.random.choice(val).replace(\"<HUMAN>\",user_data[\"User\"][\"Name\"])\n",
    "\n",
    "                print(Fore.GREEN + \"ChatBot:\" + str(tag) + Style.RESET_ALL , resp)\n",
    "                last_query = inp\n",
    "                break\n",
    "\n",
    "#         print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(responses))\n",
    "\n",
    "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_current_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
